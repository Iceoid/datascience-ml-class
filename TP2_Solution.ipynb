{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KPqlc7iw4oWJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iceoid/datascience-ml-class/blob/main/TP2_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUWung2Ki7yv"
      },
      "source": [
        "# Importing Basic Libraries\n",
        "\n",
        "`%pylab inline` imports the basic libraries (numpy, matplotlib)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Ee64K8i2GZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c06d01fc-b85c-426b-9403-fbeac3ee5ebf"
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpTTF5fxgJNS"
      },
      "source": [
        "---\n",
        "\n",
        "# Iris Flower Dataset\n",
        "\n",
        "The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "![Flowers Example](https://lh3.googleusercontent.com/proxy/sZ1CDVDWKh1ndCakWEg-milEdOSq1Cj8256LqlrNqmNHMgSTH0seFTPj-NI11dJJMiHFEGeqEWPpH7hWsKZNkN0)\n",
        "\n",
        "For our use case, we will only consider two species of Iris. (Iris versicolor and Iris virginica) This is a simple binary classification problem. $(0 = versicolor, 1 = virginica)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23LYOpZZi5qY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc9b865f-6c37-44eb-dd31-b954efd636a0"
      },
      "source": [
        "flower_classes = {\n",
        "    \"Iris-versicolor\" : 0,\n",
        "    \"Iris-virginica\" : 1,\n",
        "    \"Iris-setosa\" : 2\n",
        "}\n",
        "\n",
        "data = np.loadtxt(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", delimiter=\",\", encoding=\"ascii\", converters = {4: flower_classes.get})\n",
        "data = data[50:] # Only select versicolor and virginica flowers\n",
        "\n",
        "np.random.seed(42) # Set a constant seed for reproductibility\n",
        "np.random.shuffle(data) # Randomly shuffle the dataset\n",
        "\n",
        "print(\"Dataset shape:\", data.shape) # Print shape of dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset shape: (100, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0SxG26ph4Mz"
      },
      "source": [
        "The input data is 4-dimensional. Using numpy indexing, we can choose which features to keep. For now, we will keep all 4 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3YD-OACEgJ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "6c792c3a-f1c8-42a6-8f39-a54e722eec99"
      },
      "source": [
        "data_x = data[:, (0, 1, 2, 3)]\n",
        "data_y = data[:, -1].astype(np.uint8)\n",
        "\n",
        "print(\"First 10 entries of the dataset:\")\n",
        "print(data_x[0:10])\n",
        "print(data_y[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 entries of the dataset:\n",
            "[[6.3 2.8 5.1 1.5]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [5.  2.  3.5 1. ]\n",
            " [7.  3.2 4.7 1.4]]\n",
            "[1 1 1 0 0 0 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PKayhsIiqfq"
      },
      "source": [
        "---\n",
        "\n",
        "## Histogram Method (Classification)\n",
        "\n",
        "We can separate the input space in chunks, and make predictions based on the known data points.\n",
        "\n",
        "Implement the histogram method as described in the slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crvJjiT9v-QD"
      },
      "source": [
        "# Initializes and returns a multidimensional array of zeroes of size [divs * divs * ... * divs] (dims times)\n",
        "def initialize_histogram(dims, divs):\n",
        "    return np.zeros(np.repeat(divs, dims))\n",
        "\n",
        "# Returns the minimum and the maximum values of the input features in dataset_x\n",
        "def get_scale(dataset_x):\n",
        "    return np.min(dataset_x, axis=0), np.max(dataset_x, axis=0)\n",
        "\n",
        "# Returns the class (0 or 1) depending on the value of pred_y. Eg. 0 if smaller than 0.5, else 1\n",
        "def get_class(pred_y):\n",
        "    return np.where(pred_y < 0.5, 0, 1).item()\n",
        "\n",
        "# Takes in the number of divisions (divs), and the training dataset\n",
        "# Outputs the trained histogram, and the histogram's scale for use in prediction (same as get_scale)\n",
        "# This trained histogram should save the probability of being class 1. (Average of all labels (0 or 1) if doing binary classification)\n",
        "def train_histogram(divs, dataset_x, dataset_y):\n",
        "    dims = dataset_x.shape[1]\n",
        "    histogram_sum = initialize_histogram(dims, divs)\n",
        "    histogram_count = initialize_histogram(dims, divs)\n",
        "\n",
        "    scale_min, scale_max = get_scale(dataset_x)\n",
        "\n",
        "    for x, y in zip(dataset_x, dataset_y):\n",
        "        x_scaled = (x - scale_min) / (scale_max - scale_min)\n",
        "        x_scaled_int = tuple(np.round((x_scaled * (divs - 1))).astype(np.uint))\n",
        "        histogram_sum[x_scaled_int] += y\n",
        "        histogram_count[x_scaled_int] += 1\n",
        "\n",
        "    return histogram_sum / histogram_count, scale_min, scale_max\n",
        "\n",
        "# Takes in one unknown input, the trained histogram and the scale\n",
        "# Outputs the predicted class\n",
        "def predict_histogram(x, histogram, scale_min, scale_max):\n",
        "    divs = histogram.shape[0]\n",
        "    x_scaled = (x - scale_min) / (scale_max - scale_min)\n",
        "    x_scaled_int = tuple(np.round((x_scaled * (divs - 1))).astype(np.uint))\n",
        "    return get_class(histogram[x_scaled_int])\n",
        "\n",
        "# Takes in unknown inputs, the trained histogram and the scale\n",
        "# Outputs the array of predictions\n",
        "def predict_histogram_batch(unknowns_x, histogram, scale_min, scale_max):\n",
        "    predictions = []\n",
        "    for x in unknowns_x:\n",
        "        predictions.append(predict_histogram(x, histogram, scale_min, scale_max))\n",
        "    return np.array(predictions)\n",
        "\n",
        "# The same function can be simplified to one line!\n",
        "def predict_histogram_batch(unknowns_x, histogram, scale_min, scale_max):\n",
        "    return np.array([predict_histogram(x, histogram, scale_min, scale_max) for x in unknowns_x])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPqlc7iw4oWJ"
      },
      "source": [
        "#### Unit Tests\n",
        "These unit tests can be run to verify the histogram implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JOYL5vjwtD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3d69901-7a96-45ab-ed12-2b551d57b4db"
      },
      "source": [
        "# Unit tests for histogram\n",
        "\n",
        "\n",
        "assert initialize_histogram(3, 6).shape == (6, 6, 6)\n",
        "assert initialize_histogram(1, 1).shape == (1,)\n",
        "\n",
        "\n",
        "test_dataset_x = [(-0.5, 1, 5.1), (2.0, 0.0, 1.1), (1.2, -0.2, 3.0)]\n",
        "test_scale_min, test_scale_max = get_scale(test_dataset_x)\n",
        "\n",
        "assert (test_scale_min == [-0.5, -0.2, 1.1]).all()\n",
        "assert (test_scale_max == [ 2.0,  1.0, 5.1]).all()\n",
        "\n",
        "assert get_class(0.0) == 0\n",
        "assert get_class(1.0) == 1\n",
        "assert get_class(0.1) == 0\n",
        "assert get_class(0.3) == 0\n",
        "assert get_class(0.7) == 1\n",
        "assert get_class(0.9) == 1\n",
        "\n",
        "test_dataset_x = array([\n",
        "       [-3.90532401, -1.08369859],\n",
        "       [-3.75562443,  4.56415698],\n",
        "       [ 2.97989322, -2.41017348],\n",
        "       [ 0.88554229,  4.82330515],\n",
        "       [ 3.84198978,  1.00798446],\n",
        "       [ 4.03723967,  4.8918491 ],\n",
        "       [ 2.43352823, -4.35013591],\n",
        "       [-0.98419764,  3.37825735],\n",
        "       [-2.69430804,  3.30642518],\n",
        "       [-3.79661238, -4.53054643],\n",
        "       [-1.15909389, -4.63268063],\n",
        "       [ 4.57270263,  3.26205696],\n",
        "       [ 3.00769687,  1.29777923],\n",
        "       [-2.83675977,  0.18260367],\n",
        "       [ 0.10240073,  0.05690267]])\n",
        "\n",
        "test_dataset_y = array([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0])\n",
        "\n",
        "test_histogram, test_scale_min, test_scale_max = train_histogram(3, test_dataset_x, test_dataset_y)\n",
        "\n",
        "assert (test_histogram == array([\n",
        "        [1. , 0.5, 1. ],\n",
        "        [0.5, 0. , 0.5],\n",
        "        [0. , 0.5, 1. ]])).all()\n",
        "\n",
        "assert (test_scale_min == [-3.90532401, -4.63268063]).all()\n",
        "assert (test_scale_max == [4.57270263, 4.8918491]).all()\n",
        "\n",
        "\n",
        "test_histogram = array([\n",
        "        [1. , 0.1, 1. ],\n",
        "        [0.6, 0. , 0.3],\n",
        "        [0. , 0.1, 1. ]])\n",
        "\n",
        "test_dataset_x = array([\n",
        "       [ 4.62121973, -3.9053114 ],\n",
        "       [ 3.94479931,  4.91475335],\n",
        "       [-4.38482974,  3.82906269],\n",
        "       [ 0.1607902 ,  4.06513719],\n",
        "       [ 0.71318942,  1.6211201 ],\n",
        "       [ 0.43778908,  3.6449288 ],\n",
        "       [ 2.33909919,  0.21260312],\n",
        "       [ 3.68160804, -2.54714468],\n",
        "       [-3.4265934 , -3.39599895],\n",
        "       [-1.65991121, -2.61309969]])\n",
        "\n",
        "test_dataset_y = array([0, 1, 1, 0, 0, 0, 0, 0, 1, 1])\n",
        "\n",
        "assert predict_histogram(test_dataset_x[2], test_histogram, test_scale_min, test_scale_max) == test_dataset_y[2]\n",
        "assert predict_histogram(test_dataset_x[5], test_histogram, test_scale_min, test_scale_max) == test_dataset_y[5]\n",
        "assert predict_histogram(test_dataset_x[6], test_histogram, test_scale_min, test_scale_max) == test_dataset_y[6]\n",
        "\n",
        "assert (predict_histogram_batch(test_dataset_x, test_histogram, test_scale_min, test_scale_max) == test_dataset_y).all()\n",
        "\n",
        "print(\"Done. All tests have passed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done. All tests have passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aprJRfrOlaw3"
      },
      "source": [
        "### Histogram - Exercises\n",
        "1. Train a histogram with 5 sub-divisions per dimension and print the histogram's array (or an inner slice if the array is too big to print). What do you observe? Is data uniformly distributed within the histogram?\n",
        " * We see many empty regions (nan, from divisions by 0). This suggests the data is not uniformly distributed, as many regions have no data.\n",
        "2. What happens if you increase the number of sub-divisions to 9? What happens if you reduce it to 3 instead? Discuss.\n",
        " * Increasing the number of regions to 9 makes the histogram even emptier, and most regions that have data only contain a single data point.\n",
        " * Decreasing the number of regions to 3 reduces the empty spaces, but the histogram is still mostly empty. \n",
        " * As seen in the slides, the histogram method will not work well with this dataset as it has a 4-dimensional vector as input, while histogram methods are usually only suited for inputs that have less than 3 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxGlbqYcmH0J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "efb48b11-fe43-43d6-cc75-dd00b5c7a358"
      },
      "source": [
        "histogram_trained, histogram_min, histogram_max = train_histogram(5, data_x, data_y)\n",
        "print(histogram_trained[2, 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[       nan        nan        nan        nan        nan]\n",
            " [       nan 0.                nan        nan        nan]\n",
            " [       nan 0.14285714 0.8        1.                nan]\n",
            " [       nan        nan 1.         1.         1.        ]\n",
            " [       nan        nan        nan        nan        nan]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkqMFsx3m4Gc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "d9901f4a-ba9d-4a4d-8f90-e35e42572bf4"
      },
      "source": [
        "histogram_trained, histogram_min, histogram_max = train_histogram(9, data_x, data_y)\n",
        "print(histogram_trained[4, 4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan  0.  0. nan nan nan nan nan]\n",
            " [nan nan nan  1. nan nan nan nan nan]\n",
            " [nan nan nan nan  1.  1.  1. nan nan]\n",
            " [nan nan nan nan nan nan  1. nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSLLezmSm390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d6f1516c-6e28-4fd9-9904-f3723837773f"
      },
      "source": [
        "histogram_trained, histogram_min, histogram_max = train_histogram(3, data_x, data_y)\n",
        "print(histogram_trained[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[  nan   nan   nan]\n",
            "  [0.    0.5     nan]\n",
            "  [  nan   nan   nan]]\n",
            "\n",
            " [[0.      nan   nan]\n",
            "  [0.    0.575 1.   ]\n",
            "  [  nan   nan 1.   ]]\n",
            "\n",
            " [[  nan   nan   nan]\n",
            "  [  nan 0.    1.   ]\n",
            "  [  nan   nan   nan]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7b4YcamnLDK"
      },
      "source": [
        "3. Implement a function that counts the number of empty regions in a histogram.\n",
        "4. Knowing that a mostly empty histogram is problematic, suggest and implement a simple solution for this dataset. (Hint: Four input features is too much)\n",
        " * A simple solution can be to only use two input features. Features 0 and 2 are chosen arbritrarily, but smarter choices can be made.\n",
        "5. Train a new histogram with 5 sub-divisions using your solution and compare the resulting histogram with the one obtained in **1)** using the function implemented in **3)**.\n",
        " * There are much less empty regions in the new histogram. (578 vs 13)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7odDEgS3siEF"
      },
      "source": [
        "def count_nan(histogram):\n",
        "    return np.sum(np.isnan(histogram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogsksTMDqtcG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9cc8adaa-30b1-4861-9bdb-6169372fe6e0"
      },
      "source": [
        "data_x_selected = data[:, (0, 2)] # Select features 0 and 2\n",
        "\n",
        "histogram_trained_4, histogram_min_4, histogram_max_4 = train_histogram(5, data_x, data_y)\n",
        "print(\"4 Input Features:\", count_nan(histogram_trained_4), \"empty regions\")\n",
        "\n",
        "histogram_trained_2, histogram_min_2, histogram_max_2 = train_histogram(5, data_x_selected, data_y)\n",
        "print(\"2 Input Features:\", count_nan(histogram_trained_2), \"empty regions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 Input Features: 578 empty regions\n",
            "2 Input Features: 13 empty regions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRklqcSCts1u"
      },
      "source": [
        "6. Implement a function that computes the accuracy of predictions $f(data_x)$ by comparing them with the true values $data_y$. \n",
        "7. Using the function implemented in **6)**, note and compare the accuracy of predictions between the two histograms obtained in **1)** and **5)** on $data_x$.\n",
        "8. You should obtain a higher accuracy for the histogram with more input features. However, having many empty regions is bad. Are these two statements contradictory? How can a model have a higher accuracy and be bad at the same time? Discuss.\n",
        " * The two statements are not contradictory. Higher accuracy on the training dataset might simply mean that the histogram is memorizing the training data. This is evident when we increase the number of regions. Most regions are empty and others only contain a single point. The histogram has effectively memorized the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMG6v7nTwbW2"
      },
      "source": [
        "def accuracy(predicted_y, real_y):\n",
        "    return 1 - np.mean(np.clip(np.abs(predicted_y - real_y), 0, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7kmLopuwmY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "28513574-56e0-4c4b-e6ab-fa0a28f02b8a"
      },
      "source": [
        "print(\"4 Input Features: Accuracy of\", accuracy(predict_histogram_batch(data_x, histogram_trained_4, histogram_min_4, histogram_max_4), data_y))\n",
        "print(\"2 Input Features: Accuracy of\", accuracy(predict_histogram_batch(data_x_selected, histogram_trained_2, histogram_min_2, histogram_max_2), data_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 Input Features: Accuracy of 0.97\n",
            "2 Input Features: Accuracy of 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgxn0CtNjHi3"
      },
      "source": [
        "---\n",
        "\n",
        "## k-NN Method (Classification)\n",
        "Implement the k-NN method as described in the slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdEuldtkatc"
      },
      "source": [
        "# Distance function, returns the distance between x and y\n",
        "def distance(x, y):\n",
        "    return np.sum((x - y) ** 2, axis=-1)\n",
        "\n",
        "# Takes in one unknown input, training dataset, training labels and k\n",
        "# Outputs the predicted class\n",
        "def knn(x, dataset_x, dataset_y, k):\n",
        "    argmin_indices = argsort(distance(x, dataset_x))\n",
        "    k_nearest_neighbours = dataset_y[argmin_indices[0:k]]\n",
        "\n",
        "    class_0 = np.sum(k_nearest_neighbours == 0)\n",
        "    class_1 = np.sum(k_nearest_neighbours == 1)\n",
        "\n",
        "    if class_0 > class_1:\n",
        "        return 0\n",
        "    elif class_1 > class_0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Takes in unknown inputs, training dataset, training labels and k (optional, default = 3)\n",
        "# Outputs the array of predictions\n",
        "def knn_batch(unknowns_x, dataset_x, dataset_y, k=3):\n",
        "    return np.array([knn(x, dataset_x, dataset_y, k) for x in unknowns_x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pacp7on5WXBB"
      },
      "source": [
        "### k-NN - Exercises\n",
        "9. Using the accuracy function implemented before, note and compare the accuracy of predictions on the same dataset $data_x$ used for training when varying $k$. Try larger values of $k$ first. $(19, 7, 5, 3, ...)$ What happens when $k=1$? Discuss.\n",
        " * When $k=1$, the model is effectively returning the nearest neighbor. When testing on the same training data, it will have an accuracy of 100% as it is simply returning the training label (as the \"unknown\" point occupies the same point in the training data).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtn47gMRdT-M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6792ba7e-7e22-407c-e6ea-98912f0b9ee8"
      },
      "source": [
        "for k in [19, 7, 5, 3, 1]:\n",
        "    print(f\"k={k}: Accuracy of\", accuracy(knn_batch(data_x, data_x, data_y, k), data_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k=19: Accuracy of 0.97\n",
            "k=7: Accuracy of 0.96\n",
            "k=5: Accuracy of 0.95\n",
            "k=3: Accuracy of 0.94\n",
            "k=1: Accuracy of 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bKi8EQjK7y"
      },
      "source": [
        "---\n",
        "\n",
        "## KDE Method / Soft Parzen Windows (Classification)\n",
        "Implement the KDE method as described in the slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie3azRT-dXFF"
      },
      "source": [
        "# Gaussian kernel function with sigma s\n",
        "def kernel(x, s):\n",
        "    return np.exp(-(x**2)/(s**2)) # Generalized \"gaussian-like\" kernel equation: e^-|x|^p / s^p\n",
        "\n",
        "# Takes in one unknown input, training dataset, training labels and sigma\n",
        "# Outputs the predicted class\n",
        "def kde(x, dataset_x, dataset_y, s):\n",
        "    weights = kernel(distance(x, dataset_x), s)\n",
        "    class_0 = np.sum(weights[dataset_y == 0])\n",
        "    class_1 = np.sum(weights[dataset_y == 1])\n",
        "\n",
        "    return 0 if class_0 > class_1 else 1\n",
        "\n",
        "# Takes in unknown inputs, training dataset, training labels and sigma (optional, default = 4)\n",
        "# Outputs the array of predictions\n",
        "def kde_batch(unknowns_x, dataset_x, dataset_y, s=4):\n",
        "    return np.array([kde(x, dataset_x, dataset_y, s) for x in unknowns_x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBYWOXz9aBiB"
      },
      "source": [
        "### KDE - Exercises\n",
        "10. Using the accuracy function implemented before, note and compare the accuracy of predictions on the same dataset $data_x$ used for training when varying $\\sigma$ (variable s). Try larger values of $\\sigma$ first. $(10, 5, 1, 0.1, 0.01, ...)$ What happens when $\\sigma$ is very small? Discuss.\n",
        " * When $\\sigma$ is very small, the model gives high weights to the close neighbors. When testing on the same training data, it will have an accuracy of 100% as it is simply returning the training label (the \"unknown\" point occupies the same point in the training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EILy22QshQoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0d0d8525-d6ea-49b2-f167-62acd4c398c0"
      },
      "source": [
        "for s in [10, 5, 1, 0.1, 0.01, 0.001]:\n",
        "    print(f\"s={s}: Accuracy of\", accuracy(kde_batch(data_x, data_x, data_y, s), data_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s=10: Accuracy of 0.84\n",
            "s=5: Accuracy of 0.85\n",
            "s=1: Accuracy of 0.94\n",
            "s=0.1: Accuracy of 0.99\n",
            "s=0.01: Accuracy of 1.0\n",
            "s=0.001: Accuracy of 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}